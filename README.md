# ğŸ™ï¸ğŸ¤– ChatbotAI-Free â€” Local Voice AI Assistant

A fully **offline** voice chatbot powered by local LLMs, high-quality neural TTS, and real-time speech recognition. Practice conversations, explore ideas, or just talk â€” entirely on your own hardware, with no cloud required.

---

### âœ¨ ChatbotAI-Free in Action

**Classic Chat Mode**
<img width="1912" height="996" alt="Normal mode" src="https://github.com/user-attachments/assets/71630fb8-6b97-42fc-b4e7-3f47f736936e" />

https://github.com/user-attachments/assets/56a0d0cb-73ae-42df-8c5a-3f0938419d29

**Live Mode**
![Screenshot of Live Mode](https://github.com/user-attachments/assets/33e8d5dc-4310-4248-ba22-4b16085958b9)

https://github.com/user-attachments/assets/c33bd6a7-a4ae-48cf-89c5-c72d019a0d53

---

## ğŸš€ Features

- **ğŸŒ Multilingual TTS â€” one engine, all languages**
  [Kokoro TTS v1.0](https://github.com/thewh1teagle/kokoro-onnx) handles both **English** and **Spanish** out of the box (54 voices included). Add any additional language via a Sherpa-ONNX voice pack â€” the app auto-detects it and asks you which language it belongs to.

- **ğŸ¯ Two Conversation Modes**
  - **Classic Chat** â€” turn-by-turn, with full markdown rendering and streaming responses.
  - **Live Mode** â€” hands-free, continuous conversation with barge-in detection (interrupt the AI mid-sentence naturally).

- **ğŸ—£ï¸ Advanced Voice Pipeline**
  - Real-time Speech-to-Text via [`faster-whisper`](https://github.com/guillaumekln/faster-whisper).
  - Voice Activity Detection (VAD) for precise end-of-speech detection.
  - PipeWire-native audio playback â€” TTS never blocks other apps.

- **ğŸ§  Fully Local & Private**
  - LLM inference via [Ollama](https://ollama.ai/) â€” Llama, Mistral, Gemma, and any model you pull.
  - Streaming responses with simultaneous TTS generation.
  - Persistent conversation history with context-window indicator.

- **ğŸ“„ PDF Document Chat**
  Attach a PDF directly into the conversation â€” the app extracts text, counts tokens, and shows a detailed confirmation dialog with context-window stats before injecting it. Ask questions about the document without any external vector DB or RAG pipeline.

- **ğŸ¨ Modern, Customizable UI**
  - Dark theme inspired by Google Gemini.
  - Adjustable voice speed (0.5Ã— â€“ 2.0Ã—), font size, and audio devices.
  - Collapsible reasoning panel for thinking-capable models.

- **ğŸ” Smart Voice Scanner**
  On startup the app scans the `voices/` folder. New voice packs are detected automatically â€” you'll be prompted once to classify each one by language. No manual config needed.

---

## ğŸ› ï¸ Technology Stack

| Component | Technology |
|---|---|
| **Application & UI** | Python 3.10+, PyQt6 |
| **LLM Inference** | [Ollama](https://ollama.ai/) |
| **Speech-to-Text** | [faster-whisper](https://github.com/guillaumekln/faster-whisper) |
| **Text-to-Speech (primary)** | [Kokoro ONNX v1.0](https://github.com/thewh1teagle/kokoro-onnx) |
| **Text-to-Speech (extra voices)** | [Sherpa-ONNX](https://github.com/k2-fsa/sherpa-onnx) (optional) |
| **PDF Text Extraction** | [PyMuPDF](https://pymupdf.readthedocs.io/) |
| **Token Counting** | [tiktoken](https://github.com/openai/tiktoken) |
| **Audio I/O** | sounddevice, NumPy, paplay (PipeWire) |

---

## ğŸ“¦ Getting Started

### 1. Prerequisites

- **Python** 3.10 or 3.11
- **Ollama** installed and running â€” [ollama.ai](https://ollama.ai/)
- **Git**

### 2. Clone & Install

```bash
git clone https://github.com/maximofraisinet/ChatbotAI-Free
cd ChatbotAI-Free

python3 -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

pip install -r requirements.txt
```

For NVIDIA GPU acceleration (recommended):
```bash
pip install onnxruntime-gpu
```

### 3. Download the Kokoro v1.0 Voice Model

Kokoro powers **all built-in voices** (English + Spanish). The model files are too large for GitHub, so download them manually:

1. Go to the [kokoro-onnx releases page](https://github.com/thewh1teagle/kokoro-onnx/releases).
2. Download **`kokoro-v1.0.onnx`** and **`voices-v1.0.bin`**.
3. Place both files inside `voices/kokoro-v1.0/`:

```
voices/
â””â”€â”€ kokoro-v1.0/
    â”œâ”€â”€ kokoro-v1.0.onnx    â† ~300 MB neural TTS model
    â””â”€â”€ voices-v1.0.bin     â† ~27 MB  (54 English + Spanish voices)
```

### 4. Pull an Ollama Model

```bash
ollama pull llama3.1:8b
```

### 5. Run

```bash
python main.py
```

On first launch the voice scanner checks `voices/`. If the Kokoro files are in place you're ready to go immediately.

---

## ğŸŒ Adding More Voices (Sherpa-ONNX)

Want voices in **other languages** â€” French, Italian, German, Portuguese, and more? Use any [Piper-compatible Sherpa-ONNX VITS pack](https://huggingface.co/csukuangfj):

### Step 1 â€” Install Sherpa-ONNX

```bash
pip install sherpa-onnx
```

### Step 2 â€” Download a voice pack

Browse available voices at [huggingface.co/csukuangfj](https://huggingface.co/csukuangfj). For example, the Argentine Spanish "Daniela" voice:

```
https://huggingface.co/csukuangfj/vits-piper-es_AR-daniela-high/tree/main
```

Download these three items from the repo:
- The `.onnx` model file
- `tokens.txt`
- The `espeak-ng-data/` directory

### Step 3 â€” Drop the folder into `voices/`

Place the downloaded folder **directly** inside `voices/` (not nested deeper):

```
voices/
â”œâ”€â”€ kokoro-v1.0/                         â† built-in (Kokoro)
â”‚   â”œâ”€â”€ kokoro-v1.0.onnx
â”‚   â””â”€â”€ voices-v1.0.bin
â””â”€â”€ vits-piper-es_AR-daniela-high/       â† your new Sherpa voice
    â”œâ”€â”€ es_AR-daniela-high.onnx
    â”œâ”€â”€ tokens.txt
    â””â”€â”€ espeak-ng-data/
```

### Step 4 â€” Restart the app

On the next launch, the voice scanner detects the new folder and shows a one-time dialog asking which language to assign it to. After you confirm, the voice appears in the voice selector dropdown â€” no further setup needed.

> **Any valid Sherpa-ONNX VITS model works.** The app identifies a Sherpa pack by the presence of a `.onnx` file and an `espeak-ng-data/` sub-directory inside the folder.

---

## âŒ¨ï¸ Usage

| Control | Action |
|---|---|
| Top dropdowns | Select LLM model and active voice |
| âš™ï¸ Settings | Language, voice speed, font size, audio devices, recording mode |
| ğŸ¤ Mic button | Tap to record; tap again to send (or enable auto-send in Settings) |
| âœ¨ Live button | Enter hands-free Live Mode |
| ğŸ“ Attach button | Upload a PDF document into the conversation context |
| â¹ Stop (during playback) | Interrupt the AI mid-response |
| Context donut (bottom bar) | Click to see context window usage |

---

## ğŸ¤ Contributing

Contributions are welcome! Open an issue or submit a pull request.

1. Fork the project
2. Create your feature branch: `git checkout -b feature/AmazingFeature`
3. Commit your changes: `git commit -m 'Add AmazingFeature'`
4. Push: `git push origin feature/AmazingFeature`
5. Open a Pull Request

---

## ğŸ“„ License

This project is released under **The Unlicense**. See `LICENSE` for details.
